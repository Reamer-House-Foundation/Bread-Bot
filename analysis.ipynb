{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d36211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    ViTForImageClassification,\n",
    "    ViTFeatureExtractor,\n",
    "    ViTImageProcessor,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "import torchvision.transforms as trans\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcbf926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e94eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f832b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the image processor\n",
    "model_name_or_path: str = 'google/vit-base-patch16-224-in21k'\n",
    "cache_dir: str = None\n",
    "model_revision: str = 'main'\n",
    "use_auth_token: bool = False\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=cache_dir,\n",
    "    revision=model_revision,\n",
    "    use_auth_token=use_auth_token,\n",
    ")\n",
    "\n",
    "# Define torchvision transforms to be applied to each image.\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "_test_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a57e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_path):\n",
    "    model = ViTForImageClassification.from_pretrained(model_path, local_files_only=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "585cc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logits(model, filename, transforms=_test_transforms):\n",
    "    image = Image.open(filename)\n",
    "    \n",
    "    if image.mode in ('RGBA', 'LA') or (image.mode == 'P' and 'transparency' in image.info):\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    processed = image_processor(image)\n",
    "    processed.pixel_values = transforms(image.convert('RGB'))\n",
    "    outputs = model(torch.reshape(processed.pixel_values, (1, 3, 224, 224)))\n",
    "\n",
    "    # obtain the class\n",
    "    return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "479ff89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, filename):\n",
    "    logits = compute_logits(model, filename)\n",
    "\n",
    "    prediction = logits.argmax(-1)\n",
    "    \n",
    "    return model.config.id2label[prediction.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90aa1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_performance(model_dir, bread_test_dir='test/bread',\n",
    "                              not_bread_test_dir='test/not_bread'):\n",
    "    model_results: list[str,str,str] = [\n",
    "        #path, predicted_class, expected_class\n",
    "    ]\n",
    "    model = get_model(model_dir)\n",
    "\n",
    "    if bread_test_dir:\n",
    "        for dirpath, _, filenames in os.walk(bread_test_dir):\n",
    "            for filename in filenames:\n",
    "                path = os.path.join(dirpath, filename)\n",
    "                prediction = make_prediction(model, path)\n",
    "                model_results.append((path, prediction, 'bread'))\n",
    "\n",
    "    if not_bread_test_dir:\n",
    "        for dirpath, _, filenames in os.walk(not_bread_test_dir):\n",
    "            for filename in filenames:\n",
    "                path = os.path.join(dirpath, filename)\n",
    "                prediction = make_prediction(model, path)\n",
    "                model_results.append((path, prediction, 'not_bread'))\n",
    "            \n",
    "    true_labels = [res[2] for res in model_results]\n",
    "    pred_labels = [res[1] for res in model_results]\n",
    "\n",
    "    # Compute accuracy, precision, recall, and Jaccard score\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, pos_label='bread')\n",
    "    recall = recall_score(true_labels, pred_labels, pos_label='bread')\n",
    "    jaccard = jaccard_score(true_labels, pred_labels, pos_label='bread')\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Jaccard score:\", jaccard)\n",
    "    \n",
    "    print(f'Examples of bread:     {len([e for e in true_labels if e == \"bread\"])}')\n",
    "    print(f'Examples of not bread: {len([e for e in true_labels if e == \"not_bread\"])}')\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    print(cm)\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4e5dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_results_dir(results):\n",
    "    false_negatives = [f for f, p, a in results if p != a]\n",
    "    dirname = 'false_negatives'\n",
    "\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "\n",
    "    for file in false_negatives:\n",
    "        new_path = os.path.join(dirname, f'retrain_{os.path.basename(file)}')\n",
    "        shutil.copy(file, new_path)\n",
    "        \n",
    "        \n",
    "    dirname = 'positives'\n",
    "\n",
    "    bread = [f for f, p, a in results if p == a and p == 'bread']\n",
    "\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "\n",
    "    for file in bread:\n",
    "        new_path = os.path.join(dirname, f'retrain_{os.path.basename(file)}')\n",
    "        shutil.copy(file, new_path)\n",
    "        \n",
    "        \n",
    "    dirname = 'true_negatives'\n",
    "\n",
    "    true_negatives = [f for f, p, a in results if p == a and a != 'bread']\n",
    "\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "\n",
    "    for file in true_negatives:\n",
    "        new_path = os.path.join(dirname, f'retrain_{os.path.basename(file)}')\n",
    "        shutil.copy(file, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79c5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_data(model_name):\n",
    "    compute_model_performance(model_name, bread_test_dir='data/test', not_bread_test_dir=None)\n",
    "    \n",
    "    abs_diff = lambda t: abs(t.tolist()[0][1] - t.tolist()[0][0])\n",
    "\n",
    "    for dirpath, _, filenames in os.walk('data/test'):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(dirpath, filename)\n",
    "            tensor = compute_logits(get_model(model_name), path, transforms=_test_transforms)\n",
    "            print(path, tensor, abs_diff(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0d46b",
   "metadata": {},
   "source": [
    "## Access the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8620527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to infer channel dimension format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m original_model_results \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_model_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mcompute_model_performance\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[1;32m      9\u001b[0m         path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirpath, filename)\n\u001b[0;32m---> 10\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmake_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         model_results\u001b[38;5;241m.\u001b[39mappend((path, prediction, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbread\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirpath, _, filenames \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/not_bread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mmake_prediction\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_prediction\u001b[39m(model, filename):\n\u001b[1;32m      2\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(filename)\n\u001b[0;32m----> 4\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     processed\u001b[38;5;241m.\u001b[39mpixel_values \u001b[38;5;241m=\u001b[39m _test_transforms(image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mreshape(processed\u001b[38;5;241m.\u001b[39mpixel_values, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)))\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/image_processing_utils.py:458\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:262\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 262\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    265\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 262\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    265\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:126\u001b[0m, in \u001b[0;36mViTImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m size:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/image_transforms.py:290\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize must have 2 elements\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# For all transformations, we want to keep the same data format as the input image unless otherwise specified.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# The resized image from PIL will always have channels last, so find the input format first.\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m data_format \u001b[38;5;241m=\u001b[39m \u001b[43minfer_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data_format\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# To maintain backwards compatibility with the resizing done in previous image feature extractors, we use\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# the pillow library to resize the image and then convert back to numpy\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/image_utils.py:165\u001b[0m, in \u001b[0;36minfer_channel_dimension_format\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[last_dim] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer channel dimension format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to infer channel dimension format"
     ]
    }
   ],
   "source": [
    "original_model_results = compute_model_performance('outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6e5a6",
   "metadata": {},
   "source": [
    "## Access Kesley's Model\n",
    "#### The data was just as impure, but had a few more examples of bread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "kesley_model_results = compute_model_performance('kesley_2070_output1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc03bb",
   "metadata": {},
   "source": [
    "## Access Ray's Model\n",
    "#### For this model he cleaned up the data.  A few additional bread examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c9fc628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.767487684729064\n",
      "Precision: 1.0\n",
      "Recall: 0.7086419753086419\n",
      "Jaccard score: 0.7086419753086419\n",
      "Examples of bread:     810\n",
      "Examples of not bread: 205\n",
      "[[574 236]\n",
      " [  0 205]]\n"
     ]
    }
   ],
   "source": [
    "ray_model_results = compute_model_performance('ray_output1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "609224fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results_dir(ray_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f742b",
   "metadata": {},
   "source": [
    "## Access the first 'tuned' model\n",
    "#### This model was trained against the additional true positives and false negatives from the previous analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "360956dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9251231527093596\n",
      "Precision: 1.0\n",
      "Recall: 0.9061728395061729\n",
      "Jaccard score: 0.9061728395061729\n",
      "Examples of bread:     810\n",
      "Examples of not bread: 205\n",
      "[[734  76]\n",
      " [  0 205]]\n"
     ]
    }
   ],
   "source": [
    "tuned_model_results = compute_model_performance('tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results_dir(tuned_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408d9f6",
   "metadata": {},
   "source": [
    "### Test the first 'tuned' model against hand taken pictures of bread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9fc9b90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Jaccard score: 0.0\n",
      "Examples of bread:     6\n",
      "Examples of not bread: 0\n",
      "[[0 6]\n",
      " [0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yelsek/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('data/test/ben_bread1.jpg', 'not_bread', 'bread'),\n",
       " ('data/test/ben_bread4.jpg', 'not_bread', 'bread'),\n",
       " ('data/test/ben_bread3.jpg', 'not_bread', 'bread'),\n",
       " ('data/test/ben_bread2.jpg', 'not_bread', 'bread'),\n",
       " ('data/test/ben_bread6.jpg', 'not_bread', 'bread'),\n",
       " ('data/test/ben_bread5.jpg', 'not_bread', 'bread')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_model_performance('tuned_model', bread_test_dir='data/test', not_bread_test_dir=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c941aaa",
   "metadata": {},
   "source": [
    "### Investigate how confident the model is in its decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adee247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yelsek/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Jaccard score: 0.0\n",
      "Examples of bread:     6\n",
      "Examples of not bread: 0\n",
      "[[0 6]\n",
      " [0 0]]\n",
      "data/test/ben_bread1.jpg tensor([[-3.9181,  3.7298]], grad_fn=<AddmmBackward0>) 7.647916555404663\n",
      "data/test/ben_bread4.jpg tensor([[-4.6839,  4.6427]], grad_fn=<AddmmBackward0>) 9.326653480529785\n",
      "data/test/ben_bread3.jpg tensor([[-4.6632,  4.5624]], grad_fn=<AddmmBackward0>) 9.225627422332764\n",
      "data/test/ben_bread2.jpg tensor([[-5.2045,  5.0825]], grad_fn=<AddmmBackward0>) 10.287071704864502\n",
      "data/test/ben_bread6.jpg tensor([[-5.1308,  5.0473]], grad_fn=<AddmmBackward0>) 10.17812967300415\n",
      "data/test/ben_bread5.jpg tensor([[-5.2100,  5.0623]], grad_fn=<AddmmBackward0>) 10.272348880767822\n"
     ]
    }
   ],
   "source": [
    "evaluate_test_data('tuned_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b74d6f",
   "metadata": {},
   "source": [
    "## Investigate performance of model which applied various transformations to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8e5f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9280788177339901\n",
      "Precision: 1.0\n",
      "Recall: 0.9098765432098765\n",
      "Jaccard score: 0.9098765432098765\n",
      "Examples of bread:     810\n",
      "Examples of not bread: 205\n",
      "[[737  73]\n",
      " [  0 205]]\n"
     ]
    }
   ],
   "source": [
    "tuned_model_results = compute_model_performance('transform_test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b85011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yelsek/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Jaccard score: 0.0\n",
      "Examples of bread:     6\n",
      "Examples of not bread: 0\n",
      "[[0 6]\n",
      " [0 0]]\n",
      "data/test/ben_bread1.jpg tensor([[-4.1048,  4.0504]], grad_fn=<AddmmBackward0>) 8.155181884765625\n",
      "data/test/ben_bread4.jpg tensor([[-4.6098,  4.4740]], grad_fn=<AddmmBackward0>) 9.083780288696289\n",
      "data/test/ben_bread3.jpg tensor([[-3.6970,  3.6155]], grad_fn=<AddmmBackward0>) 7.312503099441528\n",
      "data/test/ben_bread2.jpg tensor([[-4.0864,  3.9890]], grad_fn=<AddmmBackward0>) 8.075334787368774\n",
      "data/test/ben_bread6.jpg tensor([[-4.2918,  4.2009]], grad_fn=<AddmmBackward0>) 8.492727756500244\n",
      "data/test/ben_bread5.jpg tensor([[-4.6298,  4.4756]], grad_fn=<AddmmBackward0>) 9.105485439300537\n"
     ]
    }
   ],
   "source": [
    "evaluate_test_data('transform_test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f515d6b",
   "metadata": {},
   "source": [
    "## Evaluate model with more robust transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594c3fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9310344827586207\n",
      "Precision: 1.0\n",
      "Recall: 0.9135802469135802\n",
      "Jaccard score: 0.9135802469135802\n",
      "Examples of bread:     810\n",
      "Examples of not bread: 205\n",
      "[[740  70]\n",
      " [  0 205]]\n"
     ]
    }
   ],
   "source": [
    "tuned_model_results = compute_model_performance('transform_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dd9a890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "Jaccard score: 0.5\n",
      "Examples of bread:     40\n",
      "Examples of not bread: 0\n",
      "[[20 20]\n",
      " [ 0  0]]\n",
      "data/test/DALL·E 2023-04-16 19.25.02.png tensor([[-2.3647,  2.1788]], grad_fn=<AddmmBackward0>) 4.543483734130859\n",
      "data/test/DALL·E 2023-04-16 19.24.31 - Amateur quality picture of store bought bread .png tensor([[ 3.2896, -2.9382]], grad_fn=<AddmmBackward0>) 6.227869033813477\n",
      "data/test/DALL·E 2023-04-16 19.24.39 - Amateur quality picture of store bought bread .png tensor([[ 3.5672, -3.3575]], grad_fn=<AddmmBackward0>) 6.924655437469482\n",
      "data/test/DALL·E 2023-03-27 18.25.43.png tensor([[ 3.5594, -3.3478]], grad_fn=<AddmmBackward0>) 6.907146215438843\n",
      "data/test/DALL·E 2023-04-16 19.27.31.png tensor([[ 2.0394, -1.8505]], grad_fn=<AddmmBackward0>) 3.8899019956588745\n",
      "data/test/ben_bread1.jpg tensor([[-4.1550,  3.9975]], grad_fn=<AddmmBackward0>) 8.152436256408691\n",
      "data/test/DALL·E 2023-04-16 19.24.23 - Amateur quality picture of wonderbread .png tensor([[ 2.7590, -2.4392]], grad_fn=<AddmmBackward0>) 5.198197841644287\n",
      "data/test/DALL·E 2023-04-16 19.24.54.png tensor([[-3.8163,  3.5695]], grad_fn=<AddmmBackward0>) 7.38582444190979\n",
      "data/test/ben_bread4.jpg tensor([[-4.0742,  3.9287]], grad_fn=<AddmmBackward0>) 8.00290822982788\n",
      "data/test/DALL·E 2023-04-16 19.36.50.png tensor([[ 2.2901, -2.0857]], grad_fn=<AddmmBackward0>) 4.37575626373291\n",
      "data/test/DALL·E 2023-03-27 18.25.48.png tensor([[ 3.5710, -3.3356]], grad_fn=<AddmmBackward0>) 6.906581163406372\n",
      "data/test/DALL·E 2023-04-16 19.40.29.png tensor([[ 3.0937, -2.9357]], grad_fn=<AddmmBackward0>) 6.029456853866577\n",
      "data/test/DALL·E 2023-04-16 19.29.44.png tensor([[-4.2866,  4.1548]], grad_fn=<AddmmBackward0>) 8.44141435623169\n",
      "data/test/DALL·E 2023-04-16 19.27.28.png tensor([[-1.7068,  1.4319]], grad_fn=<AddmmBackward0>) 3.1387336254119873\n",
      "data/test/DALL·E 2023-04-16 19.40.31.png tensor([[ 1.9959, -1.8478]], grad_fn=<AddmmBackward0>) 3.8436527252197266\n",
      "data/test/DALL·E 2023-04-16 19.40.35.png tensor([[ 1.8781, -1.8127]], grad_fn=<AddmmBackward0>) 3.690722942352295\n",
      "data/test/DALL·E 2023-04-16 19.29.43.png tensor([[-2.8189,  2.6538]], grad_fn=<AddmmBackward0>) 5.472740173339844\n",
      "data/test/DALL·E 2023-03-27 18.25.41.png tensor([[ 3.5953, -3.3632]], grad_fn=<AddmmBackward0>) 6.958497047424316\n",
      "data/test/ben_bread3.jpg tensor([[-4.4197,  4.2643]], grad_fn=<AddmmBackward0>) 8.683994770050049\n",
      "data/test/DALL·E 2023-04-16 19.36.55.png tensor([[ 2.8094, -2.6993]], grad_fn=<AddmmBackward0>) 5.508697271347046\n",
      "data/test/DALL·E 2023-04-16 19.29.39.png tensor([[-3.8434,  3.6355]], grad_fn=<AddmmBackward0>) 7.478879928588867\n",
      "data/test/DALL·E 2023-04-16 19.25.00.png tensor([[-3.3243,  3.0311]], grad_fn=<AddmmBackward0>) 6.355410575866699\n",
      "data/test/ben_bread2.jpg tensor([[-4.2488,  4.0859]], grad_fn=<AddmmBackward0>) 8.334743976593018\n",
      "data/test/DALL·E 2023-04-16 19.40.32.png tensor([[-1.7877,  1.2453]], grad_fn=<AddmmBackward0>) 3.033003568649292\n",
      "data/test/DALL·E 2023-04-16 19.36.52.png tensor([[ 0.4398, -0.6673]], grad_fn=<AddmmBackward0>) 1.1071366369724274\n",
      "data/test/DALL·E 2023-04-16 19.24.56.png tensor([[-2.4136,  2.1140]], grad_fn=<AddmmBackward0>) 4.527527093887329\n",
      "data/test/DALL·E 2023-03-27 18.24.53 - Amateur quality picture of store bought bread_.png tensor([[ 3.5989, -3.3483]], grad_fn=<AddmmBackward0>) 6.9472033977508545\n",
      "data/test/ben_bread6.jpg tensor([[-4.1241,  3.9767]], grad_fn=<AddmmBackward0>) 8.100765705108643\n",
      "data/test/DALL·E 2023-04-16 19.29.40.png tensor([[-4.0211,  3.8434]], grad_fn=<AddmmBackward0>) 7.864486455917358\n",
      "data/test/DALL·E 2023-03-27 18.24.50 - Amateur quality picture of store bought bread_.png tensor([[ 3.5672, -3.3575]], grad_fn=<AddmmBackward0>) 6.924655437469482\n",
      "data/test/DALL·E 2023-04-16 19.24.35 - Amateur quality picture of store bought bread .png tensor([[ 3.0057, -2.7869]], grad_fn=<AddmmBackward0>) 5.792687654495239\n",
      "data/test/DALL·E 2023-04-16 19.28.23.png tensor([[-3.8789,  3.6733]], grad_fn=<AddmmBackward0>) 7.552196025848389\n",
      "data/test/DALL·E 2023-04-16 19.27.33.png tensor([[ 0.5545, -0.5267]], grad_fn=<AddmmBackward0>) 1.0811994671821594\n",
      "data/test/ben_bread5.jpg tensor([[-4.2059,  3.9865]], grad_fn=<AddmmBackward0>) 8.19242000579834\n",
      "data/test/DALL·E 2023-04-16 19.28.18.png tensor([[ 1.4344, -1.1854]], grad_fn=<AddmmBackward0>) 2.619760274887085\n",
      "data/test/DALL·E 2023-03-27 18.24.22 - Amateur quality picture of wonderbread .png tensor([[ 3.5713, -3.3559]], grad_fn=<AddmmBackward0>) 6.9271769523620605\n",
      "data/test/DALL·E 2023-04-16 19.36.58.png tensor([[ 3.4009, -3.1807]], grad_fn=<AddmmBackward0>) 6.58165717124939\n",
      "data/test/DALL·E 2023-04-16 19.27.35.png tensor([[-4.1714,  4.0179]], grad_fn=<AddmmBackward0>) 8.189356803894043\n",
      "data/test/DALL·E 2023-04-16 19.28.30.png tensor([[-4.2915,  4.1069]], grad_fn=<AddmmBackward0>) 8.39843463897705\n",
      "data/test/DALL·E 2023-04-16 19.28.27.png tensor([[-4.2215,  4.0663]], grad_fn=<AddmmBackward0>) 8.28775691986084\n"
     ]
    }
   ],
   "source": [
    "evaluate_test_data('transform_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dce4ebe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6153846153846154\n",
      "Precision: 1.0\n",
      "Recall: 0.6153846153846154\n",
      "Jaccard score: 0.6153846153846154\n",
      "Examples of bread:     52\n",
      "Examples of not bread: 0\n",
      "[[32 20]\n",
      " [ 0  0]]\n",
      "data/test/DALL·E 2023-04-16 19.25.02.png tensor([[ 2.8928, -2.5842]], grad_fn=<AddmmBackward0>) 5.476934432983398\n",
      "data/test/494.png tensor([[-4.0470,  3.7243]], grad_fn=<AddmmBackward0>) 7.771250247955322\n",
      "data/test/DALL·E 2023-04-16 19.24.31 - Amateur quality picture of store bought bread .png tensor([[ 3.1003, -2.8235]], grad_fn=<AddmmBackward0>) 5.923765659332275\n",
      "data/test/DALL·E 2023-04-16 19.24.39 - Amateur quality picture of store bought bread .png tensor([[ 3.6781, -3.3804]], grad_fn=<AddmmBackward0>) 7.058478116989136\n",
      "data/test/DALL·E 2023-03-27 18.25.43.png tensor([[ 3.6981, -3.3784]], grad_fn=<AddmmBackward0>) 7.076551675796509\n",
      "data/test/DALL·E 2023-04-16 19.27.31.png tensor([[ 3.1161, -2.8003]], grad_fn=<AddmmBackward0>) 5.916431188583374\n",
      "data/test/ben_bread1.jpg tensor([[-2.3267,  2.0766]], grad_fn=<AddmmBackward0>) 4.403329610824585\n",
      "data/test/DALL·E 2023-04-16 19.24.23 - Amateur quality picture of wonderbread .png tensor([[ 3.1784, -2.8802]], grad_fn=<AddmmBackward0>) 6.0585596561431885\n",
      "data/test/DALL·E 2023-04-16 19.24.54.png tensor([[ 2.5376, -2.2018]], grad_fn=<AddmmBackward0>) 4.739392042160034\n",
      "data/test/ben_bread4.jpg tensor([[-2.2590,  2.0617]], grad_fn=<AddmmBackward0>) 4.320671558380127\n",
      "data/test/DALL·E 2023-04-16 19.36.50.png tensor([[ 1.9807, -1.7080]], grad_fn=<AddmmBackward0>) 3.688716411590576\n",
      "data/test/2118.png tensor([[-1.4645,  1.2904]], grad_fn=<AddmmBackward0>) 2.754992961883545\n",
      "data/test/DALL·E 2023-03-27 18.25.48.png tensor([[ 3.6307, -3.3029]], grad_fn=<AddmmBackward0>) 6.933570146560669\n",
      "data/test/DALL·E 2023-04-16 19.40.29.png tensor([[ 3.2303, -2.8718]], grad_fn=<AddmmBackward0>) 6.102069139480591\n",
      "data/test/DALL·E 2023-04-16 19.29.44.png tensor([[ 3.0773, -2.7440]], grad_fn=<AddmmBackward0>) 5.821277379989624\n",
      "data/test/DALL·E 2023-04-16 19.27.28.png tensor([[ 3.2426, -2.8830]], grad_fn=<AddmmBackward0>) 6.125607013702393\n",
      "data/test/DALL·E 2023-04-16 19.40.31.png tensor([[ 1.6666, -1.4229]], grad_fn=<AddmmBackward0>) 3.0894752740859985\n",
      "data/test/DALL·E 2023-04-16 19.40.35.png tensor([[ 1.9843, -1.7327]], grad_fn=<AddmmBackward0>) 3.717058777809143\n",
      "data/test/DALL·E 2023-04-16 19.29.43.png tensor([[ 2.7384, -2.4066]], grad_fn=<AddmmBackward0>) 5.14496922492981\n",
      "data/test/DALL·E 2023-03-27 18.25.41.png tensor([[ 3.6322, -3.3021]], grad_fn=<AddmmBackward0>) 6.934316873550415\n",
      "data/test/ben_bread3.jpg tensor([[-1.9032,  1.6975]], grad_fn=<AddmmBackward0>) 3.6007155179977417\n",
      "data/test/DALL·E 2023-04-16 19.36.55.png tensor([[ 1.9334, -1.7060]], grad_fn=<AddmmBackward0>) 3.6394448280334473\n",
      "data/test/44.png tensor([[-2.1007,  1.8510]], grad_fn=<AddmmBackward0>) 3.951788544654846\n",
      "data/test/DALL·E 2023-04-16 19.29.39.png tensor([[ 2.9156, -2.5560]], grad_fn=<AddmmBackward0>) 5.471628904342651\n",
      "data/test/594.png tensor([[-1.5714,  1.4796]], grad_fn=<AddmmBackward0>) 3.0510226488113403\n",
      "data/test/1132.png tensor([[-3.5818,  3.3200]], grad_fn=<AddmmBackward0>) 6.901745319366455\n",
      "data/test/DALL·E 2023-04-16 19.25.00.png tensor([[ 3.0799, -2.7415]], grad_fn=<AddmmBackward0>) 5.821377277374268\n",
      "data/test/ben_bread2.jpg tensor([[-1.7313,  1.6206]], grad_fn=<AddmmBackward0>) 3.351825714111328\n",
      "data/test/2007.png tensor([[-1.2551,  1.0134]], grad_fn=<AddmmBackward0>) 2.268541932106018\n",
      "data/test/DALL·E 2023-04-16 19.40.32.png tensor([[ 1.1434, -1.0581]], grad_fn=<AddmmBackward0>) 2.201483368873596\n",
      "data/test/DALL·E 2023-04-16 19.36.52.png tensor([[ 2.4370, -2.2296]], grad_fn=<AddmmBackward0>) 4.666607856750488\n",
      "data/test/DALL·E 2023-04-16 19.24.56.png tensor([[ 3.0809, -2.7346]], grad_fn=<AddmmBackward0>) 5.815458059310913\n",
      "data/test/DALL·E 2023-03-27 18.24.53 - Amateur quality picture of store bought bread_.png tensor([[ 3.6675, -3.3179]], grad_fn=<AddmmBackward0>) 6.9854185581207275\n",
      "data/test/ben_bread6.jpg tensor([[-2.7923,  2.5034]], grad_fn=<AddmmBackward0>) 5.295679569244385\n",
      "data/test/1515.png tensor([[-4.7889,  4.5841]], grad_fn=<AddmmBackward0>) 9.373067855834961\n",
      "data/test/DALL·E 2023-04-16 19.29.40.png tensor([[ 2.5024, -2.2028]], grad_fn=<AddmmBackward0>) 4.705221176147461\n",
      "data/test/DALL·E 2023-03-27 18.24.50 - Amateur quality picture of store bought bread_.png tensor([[ 3.6781, -3.3804]], grad_fn=<AddmmBackward0>) 7.058478116989136\n",
      "data/test/DALL·E 2023-04-16 19.24.35 - Amateur quality picture of store bought bread .png tensor([[ 3.0538, -2.7417]], grad_fn=<AddmmBackward0>) 5.795572996139526\n",
      "data/test/DALL·E 2023-04-16 19.28.23.png tensor([[-0.4088,  0.3887]], grad_fn=<AddmmBackward0>) 0.7975210547447205\n",
      "data/test/1078.png tensor([[-0.7848,  0.8368]], grad_fn=<AddmmBackward0>) 1.6215577721595764\n",
      "data/test/1123.png tensor([[-1.7547,  1.5180]], grad_fn=<AddmmBackward0>) 3.272681951522827\n",
      "data/test/1509.png tensor([[-2.7791,  2.6959]], grad_fn=<AddmmBackward0>) 5.474938631057739\n",
      "data/test/DALL·E 2023-04-16 19.27.33.png tensor([[ 3.1530, -2.8275]], grad_fn=<AddmmBackward0>) 5.980491876602173\n",
      "data/test/ben_bread5.jpg tensor([[-2.2858,  2.0094]], grad_fn=<AddmmBackward0>) 4.295278787612915\n",
      "data/test/DALL·E 2023-04-16 19.28.18.png tensor([[ 2.7552, -2.4450]], grad_fn=<AddmmBackward0>) 5.200196266174316\n",
      "data/test/DALL·E 2023-03-27 18.24.22 - Amateur quality picture of wonderbread .png tensor([[ 3.6073, -3.3035]], grad_fn=<AddmmBackward0>) 6.910865545272827\n",
      "data/test/909.png tensor([[-1.2656,  1.1018]], grad_fn=<AddmmBackward0>) 2.367493510246277\n",
      "data/test/DALL·E 2023-04-16 19.36.58.png tensor([[ 3.3451, -3.0318]], grad_fn=<AddmmBackward0>) 6.376888036727905\n",
      "data/test/DALL·E 2023-04-16 19.27.35.png tensor([[ 2.0449, -1.8150]], grad_fn=<AddmmBackward0>) 3.8598978519439697\n",
      "data/test/DALL·E 2023-04-16 19.28.30.png tensor([[-2.4384,  2.2153]], grad_fn=<AddmmBackward0>) 4.653668403625488\n",
      "data/test/1077.png tensor([[-2.9480,  2.7176]], grad_fn=<AddmmBackward0>) 5.6655800342559814\n",
      "data/test/DALL·E 2023-04-16 19.28.27.png tensor([[ 1.8481, -1.6930]], grad_fn=<AddmmBackward0>) 3.541046619415283\n"
     ]
    }
   ],
   "source": [
    "evaluate_test_data('transform_gan1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e4c98ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9399014778325123\n",
      "Precision: 0.9986684420772304\n",
      "Recall: 0.9259259259259259\n",
      "Jaccard score: 0.9247842170160296\n",
      "Examples of bread:     810\n",
      "Examples of not bread: 205\n",
      "[[750  60]\n",
      " [  1 204]]\n"
     ]
    }
   ],
   "source": [
    "tuned_model_results = compute_model_performance('transform_gan2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5ace16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7884615384615384\n",
      "Precision: 1.0\n",
      "Recall: 0.7884615384615384\n",
      "Jaccard score: 0.7884615384615384\n",
      "Examples of bread:     52\n",
      "Examples of not bread: 0\n",
      "[[41 11]\n",
      " [ 0  0]]\n",
      "data/test/DALL·E 2023-04-16 19.25.02.png tensor([[ 3.2798, -3.0134]], grad_fn=<AddmmBackward0>) 6.293270826339722\n",
      "data/test/494.png tensor([[-3.6405,  3.3382]], grad_fn=<AddmmBackward0>) 6.978690147399902\n",
      "data/test/DALL·E 2023-04-16 19.24.31 - Amateur quality picture of store bought bread .png tensor([[ 3.2421, -2.9930]], grad_fn=<AddmmBackward0>) 6.235102415084839\n",
      "data/test/DALL·E 2023-04-16 19.24.39 - Amateur quality picture of store bought bread .png tensor([[ 3.4635, -3.1351]], grad_fn=<AddmmBackward0>) 6.598620891571045\n",
      "data/test/DALL·E 2023-03-27 18.25.43.png tensor([[ 3.5058, -3.1913]], grad_fn=<AddmmBackward0>) 6.697089433670044\n",
      "data/test/DALL·E 2023-04-16 19.27.31.png tensor([[ 3.2895, -2.9680]], grad_fn=<AddmmBackward0>) 6.257474184036255\n",
      "data/test/ben_bread1.jpg tensor([[-2.0123,  1.8067]], grad_fn=<AddmmBackward0>) 3.819033622741699\n",
      "data/test/DALL·E 2023-04-16 19.24.23 - Amateur quality picture of wonderbread .png tensor([[ 3.1976, -2.9375]], grad_fn=<AddmmBackward0>) 6.135044097900391\n",
      "data/test/DALL·E 2023-04-16 19.24.54.png tensor([[ 2.8722, -2.6364]], grad_fn=<AddmmBackward0>) 5.508675813674927\n",
      "data/test/ben_bread4.jpg tensor([[-1.6270,  1.5518]], grad_fn=<AddmmBackward0>) 3.1788305044174194\n",
      "data/test/DALL·E 2023-04-16 19.36.50.png tensor([[ 2.5626, -2.2877]], grad_fn=<AddmmBackward0>) 4.850310564041138\n",
      "data/test/2118.png tensor([[ 2.8538, -2.5308]], grad_fn=<AddmmBackward0>) 5.3845908641815186\n",
      "data/test/DALL·E 2023-03-27 18.25.48.png tensor([[ 3.3893, -3.0918]], grad_fn=<AddmmBackward0>) 6.481074810028076\n",
      "data/test/DALL·E 2023-04-16 19.40.29.png tensor([[ 3.2773, -2.9397]], grad_fn=<AddmmBackward0>) 6.216956377029419\n",
      "data/test/DALL·E 2023-04-16 19.29.44.png tensor([[ 3.1824, -2.8297]], grad_fn=<AddmmBackward0>) 6.012047529220581\n",
      "data/test/DALL·E 2023-04-16 19.27.28.png tensor([[ 3.1372, -2.8172]], grad_fn=<AddmmBackward0>) 5.954348564147949\n",
      "data/test/DALL·E 2023-04-16 19.40.31.png tensor([[ 3.0369, -2.6941]], grad_fn=<AddmmBackward0>) 5.730955362319946\n",
      "data/test/DALL·E 2023-04-16 19.40.35.png tensor([[ 3.2205, -2.8403]], grad_fn=<AddmmBackward0>) 6.060837507247925\n",
      "data/test/DALL·E 2023-04-16 19.29.43.png tensor([[ 2.4895, -2.2496]], grad_fn=<AddmmBackward0>) 4.739065885543823\n",
      "data/test/DALL·E 2023-03-27 18.25.41.png tensor([[ 3.2918, -3.0106]], grad_fn=<AddmmBackward0>) 6.302399396896362\n",
      "data/test/ben_bread3.jpg tensor([[ 1.9959, -1.7438]], grad_fn=<AddmmBackward0>) 3.7397249937057495\n",
      "data/test/DALL·E 2023-04-16 19.36.55.png tensor([[ 2.0976, -1.7889]], grad_fn=<AddmmBackward0>) 3.886466860771179\n",
      "data/test/44.png tensor([[ 2.1880, -1.9141]], grad_fn=<AddmmBackward0>) 4.102083444595337\n",
      "data/test/DALL·E 2023-04-16 19.29.39.png tensor([[ 3.0764, -2.7864]], grad_fn=<AddmmBackward0>) 5.862836837768555\n",
      "data/test/594.png tensor([[ 2.9062, -2.6068]], grad_fn=<AddmmBackward0>) 5.513015985488892\n",
      "data/test/1132.png tensor([[-1.2116,  1.3154]], grad_fn=<AddmmBackward0>) 2.5270575284957886\n",
      "data/test/DALL·E 2023-04-16 19.25.00.png tensor([[ 3.1433, -2.8747]], grad_fn=<AddmmBackward0>) 6.018026828765869\n",
      "data/test/ben_bread2.jpg tensor([[-0.7397,  0.8350]], grad_fn=<AddmmBackward0>) 1.5747368335723877\n",
      "data/test/2007.png tensor([[ 2.5298, -2.2589]], grad_fn=<AddmmBackward0>) 4.788654088973999\n",
      "data/test/DALL·E 2023-04-16 19.40.32.png tensor([[ 3.2775, -2.9486]], grad_fn=<AddmmBackward0>) 6.226111888885498\n",
      "data/test/DALL·E 2023-04-16 19.36.52.png tensor([[ 2.8071, -2.5099]], grad_fn=<AddmmBackward0>) 5.317018508911133\n",
      "data/test/DALL·E 2023-04-16 19.24.56.png tensor([[ 3.3171, -3.0510]], grad_fn=<AddmmBackward0>) 6.368058204650879\n",
      "data/test/DALL·E 2023-03-27 18.24.53 - Amateur quality picture of store bought bread_.png tensor([[ 3.3814, -3.1165]], grad_fn=<AddmmBackward0>) 6.497887134552002\n",
      "data/test/ben_bread6.jpg tensor([[-2.4640,  2.3357]], grad_fn=<AddmmBackward0>) 4.799668550491333\n",
      "data/test/1515.png tensor([[-4.3086,  4.1174]], grad_fn=<AddmmBackward0>) 8.425963878631592\n",
      "data/test/DALL·E 2023-04-16 19.29.40.png tensor([[ 3.0389, -2.7683]], grad_fn=<AddmmBackward0>) 5.807149410247803\n",
      "data/test/DALL·E 2023-03-27 18.24.50 - Amateur quality picture of store bought bread_.png tensor([[ 3.4635, -3.1351]], grad_fn=<AddmmBackward0>) 6.598620891571045\n",
      "data/test/DALL·E 2023-04-16 19.24.35 - Amateur quality picture of store bought bread .png tensor([[ 3.1969, -2.9420]], grad_fn=<AddmmBackward0>) 6.138902187347412\n",
      "data/test/DALL·E 2023-04-16 19.28.23.png tensor([[ 1.3754, -1.1560]], grad_fn=<AddmmBackward0>) 2.5314329862594604\n",
      "data/test/1078.png tensor([[ 2.6017, -2.2901]], grad_fn=<AddmmBackward0>) 4.8918297290802\n",
      "data/test/1123.png tensor([[-2.5611,  2.3970]], grad_fn=<AddmmBackward0>) 4.9580981731414795\n",
      "data/test/1509.png tensor([[ 2.2599, -2.0465]], grad_fn=<AddmmBackward0>) 4.306421995162964\n",
      "data/test/DALL·E 2023-04-16 19.27.33.png tensor([[ 3.0674, -2.7521]], grad_fn=<AddmmBackward0>) 5.819556474685669\n",
      "data/test/ben_bread5.jpg tensor([[-2.4441,  2.1695]], grad_fn=<AddmmBackward0>) 4.613685846328735\n",
      "data/test/DALL·E 2023-04-16 19.28.18.png tensor([[ 2.9267, -2.5858]], grad_fn=<AddmmBackward0>) 5.512544870376587\n",
      "data/test/DALL·E 2023-03-27 18.24.22 - Amateur quality picture of wonderbread .png tensor([[ 3.3624, -3.0363]], grad_fn=<AddmmBackward0>) 6.398693561553955\n",
      "data/test/909.png tensor([[ 2.5076, -2.2686]], grad_fn=<AddmmBackward0>) 4.776203155517578\n",
      "data/test/DALL·E 2023-04-16 19.36.58.png tensor([[ 3.0357, -2.6953]], grad_fn=<AddmmBackward0>) 5.731015205383301\n",
      "data/test/DALL·E 2023-04-16 19.27.35.png tensor([[ 2.4860, -2.2220]], grad_fn=<AddmmBackward0>) 4.708038806915283\n",
      "data/test/DALL·E 2023-04-16 19.28.30.png tensor([[-2.2549,  2.0674]], grad_fn=<AddmmBackward0>) 4.322263479232788\n",
      "data/test/1077.png tensor([[-0.3445,  0.3428]], grad_fn=<AddmmBackward0>) 0.6872985064983368\n",
      "data/test/DALL·E 2023-04-16 19.28.27.png tensor([[ 1.9762, -1.7904]], grad_fn=<AddmmBackward0>) 3.766626238822937\n"
     ]
    }
   ],
   "source": [
    "evaluate_test_data('transform_gan2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea79c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7884615384615384\n",
      "Precision: 1.0\n",
      "Recall: 0.7884615384615384\n",
      "Jaccard score: 0.7884615384615384\n",
      "Examples of bread:     52\n",
      "Examples of not bread: 0\n",
      "[[41 11]\n",
      " [ 0  0]]\n"
     ]
    }
   ],
   "source": [
    "tuned_model_results = compute_model_performance('transform_gan2', bread_test_dir='data/test', not_bread_test_dir=None)\n",
    "group_results_dir(tuned_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f30bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
