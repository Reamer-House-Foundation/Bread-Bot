{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d36211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    ViTForImageClassification,\n",
    "    ViTFeatureExtractor,\n",
    "    ViTImageProcessor,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbf926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e94eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f832b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the image processor\n",
    "model_name_or_path: str = 'google/vit-base-patch16-224-in21k'\n",
    "cache_dir: str = None\n",
    "model_revision: str = 'main'\n",
    "use_auth_token: bool = False\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=cache_dir,\n",
    "    revision=model_revision,\n",
    "    use_auth_token=use_auth_token,\n",
    ")\n",
    "\n",
    "# Define torchvision transforms to be applied to each image.\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "_test_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a57e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_path):\n",
    "    model = ViTForImageClassification.from_pretrained(model_path, local_files_only=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479ff89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, filename):\n",
    "    image = Image.open(filename)\n",
    "\n",
    "    processed = image_processor(image)\n",
    "    processed.pixel_values = _test_transforms(image.convert('RGB'))\n",
    "    outputs = model(torch.reshape(processed.pixel_values, (1, 3, 224, 224)))\n",
    "\n",
    "    # obtain the class\n",
    "    logits = outputs.logits\n",
    "\n",
    "    prediction = logits.argmax(-1)\n",
    "    \n",
    "    return model.config.id2label[prediction.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90aa1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_performance(model_dir):\n",
    "    model_results: list[str,str,str] = [\n",
    "        #path, predicted_class, expected_class\n",
    "    ]\n",
    "    model = get_model(model_dir)\n",
    "\n",
    "    for dirpath, _, filenames in os.walk('test/bread'):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(dirpath, filename)\n",
    "            prediction = make_prediction(model, path)\n",
    "            model_results.append((path, prediction, 'bread'))\n",
    "\n",
    "    for dirpath, _, filenames in os.walk('test/not_bread'):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(dirpath, filename)\n",
    "            prediction = make_prediction(model, path)\n",
    "            model_results.append((path, prediction, 'not_bread'))\n",
    "            \n",
    "    true_labels = [res[2] for res in model_results]\n",
    "    pred_labels = [res[1] for res in model_results]\n",
    "\n",
    "    # Compute accuracy, precision, recall, and Jaccard score\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, pos_label='bread')\n",
    "    recall = recall_score(true_labels, pred_labels, pos_label='bread')\n",
    "    jaccard = jaccard_score(true_labels, pred_labels, pos_label='bread')\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Jaccard score:\", jaccard)\n",
    "    \n",
    "    print(f'Examples of bread:     {len([e for e in true_labels if e == \"bread\"])}')\n",
    "    print(f'Examples of not bread: {len([e for e in true_labels if e == \"not_bread\"])}')\n",
    "    \n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    print(cm)\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0d46b",
   "metadata": {},
   "source": [
    "## Access the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8620527",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to infer channel dimension format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m original_model_results \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_model_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mcompute_model_performance\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[1;32m      9\u001b[0m         path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirpath, filename)\n\u001b[0;32m---> 10\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmake_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         model_results\u001b[38;5;241m.\u001b[39mappend((path, prediction, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbread\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirpath, _, filenames \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/not_bread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mmake_prediction\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_prediction\u001b[39m(model, filename):\n\u001b[1;32m      2\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(filename)\n\u001b[0;32m----> 4\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     processed\u001b[38;5;241m.\u001b[39mpixel_values \u001b[38;5;241m=\u001b[39m _test_transforms(image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mreshape(processed\u001b[38;5;241m.\u001b[39mpixel_values, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)))\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/image_processing_utils.py:458\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    457\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:262\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 262\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    265\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 262\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    265\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:126\u001b[0m, in \u001b[0;36mViTImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m size:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/image_transforms.py:290\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize must have 2 elements\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# For all transformations, we want to keep the same data format as the input image unless otherwise specified.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# The resized image from PIL will always have channels last, so find the input format first.\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m data_format \u001b[38;5;241m=\u001b[39m \u001b[43minfer_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data_format\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# To maintain backwards compatibility with the resizing done in previous image feature extractors, we use\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# the pillow library to resize the image and then convert back to numpy\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n",
      "File \u001b[0;32m~/projects/Bread-Bot/bread_env/lib64/python3.11/site-packages/transformers/image_utils.py:165\u001b[0m, in \u001b[0;36minfer_channel_dimension_format\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[last_dim] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer channel dimension format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to infer channel dimension format"
     ]
    }
   ],
   "source": [
    "original_model_results = compute_model_performance('outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6e5a6",
   "metadata": {},
   "source": [
    "## Access Kesley's Model\n",
    "#### The data was just as impure, but had a few more examples of bread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "kesley_model_results = compute_model_performance('kesley_2070_output1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc03bb",
   "metadata": {},
   "source": [
    "## Access Ray's Model\n",
    "#### For this model he cleaned up the data.  A few additional bread examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_model_results = compute_model_performance('ray_output1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0f4b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "false_negatives = [f for f, p, a in ray_model_results if p != a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'false_negatives'\n",
    "\n",
    "if not os.path.isdir(dirname):\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "for file in false_negatives:\n",
    "    new_path = os.path.join(dirname, os.path.basename(file))\n",
    "    shutil.copy(file, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'positives'\n",
    "\n",
    "bread = [f for f, p, a in ray_model_results if p == a and p == 'bread']\n",
    "\n",
    "if not os.path.isdir(dirname):\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "for file in bread:\n",
    "    new_path = os.path.join(dirname, os.path.basename(file))\n",
    "    shutil.copy(file, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252439d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
